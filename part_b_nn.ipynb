{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np \n",
    "import sys\n",
    "import pdb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import multiprocessing\n",
    "plt.style.use('ggplot')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(x_path, y_path):\n",
    "    '''\n",
    "    Args:\n",
    "        x_path: path to x file\n",
    "        y_path: path to y file\n",
    "    Returns:\n",
    "        x: np array of [NUM_OF_SAMPLES x n]\n",
    "        y: np array of [NUM_OF_SAMPLES]\n",
    "    '''\n",
    "    x = np.load(x_path)\n",
    "    y = np.load(y_path)\n",
    "\n",
    "    y = y.astype('float')\n",
    "    x = x.astype('float')\n",
    "\n",
    "    #normalize x:\n",
    "    x = 2*(0.5 - x/255)\n",
    "    return x, y\n",
    "\n",
    "def get_metric(y_true, y_pred):\n",
    "    '''\n",
    "    Args:\n",
    "        y_true: np array of [NUM_SAMPLES x r] (one hot) \n",
    "                or np array of [NUM_SAMPLES]\n",
    "        y_pred: np array of [NUM_SAMPLES x r] (one hot) \n",
    "                or np array of [NUM_SAMPLES]\n",
    "                \n",
    "    '''\n",
    "    results = classification_report(y_pred, y_true)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer(ABC):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights =  np.random.normal(0, 0.01, (input_size + 1, output_size)) # Includes bias\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        # self.bias = np.random.rand(output_size, 1)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def activation(self, input):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input_data_with_bias = np.hstack((np.ones((input.shape[0],1)), input))\n",
    "        self.input = input_data_with_bias\n",
    "        \n",
    "        z = np.dot(input_data_with_bias, self.weights)\n",
    "        \n",
    "        self.output = self.activation(z)\n",
    "        \n",
    "        # activation = 1/(1+np.exp(-(np.dot(input_data_with_bias, self.weights))))\n",
    "        return self.output\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidLayer(BaseLayer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__(input_size, output_size)\n",
    "    \n",
    "    def activation(self, input_data):\n",
    "        return 1/(1 + np.exp(-input_data))\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_input = grad_output * self.output * (1 - self.output)\n",
    "        grad_weights = np.dot(self.input.T, grad_input)\n",
    "        \n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        \n",
    "        weight_with_bias = self.weights[1:, :]\n",
    "        return np.dot(grad_input, weight_with_bias.T)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluLayer(BaseLayer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__(input_size, output_size)\n",
    "    \n",
    "    def activation(self, input_data):\n",
    "        return np.maximum(0.0, input_data)\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_relu = (self.output > 0).astype(float)\n",
    "        grad_input = grad_output * grad_relu\n",
    "        grad_w = np.dot(self.input.T, grad_input)\n",
    "\n",
    "        # Update weights\n",
    "        self.weights -= learning_rate * grad_w\n",
    "\n",
    "        weight_with_bias = self.weights[1:, :]\n",
    "        return np.dot(grad_input, weight_with_bias.T)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxLayer(BaseLayer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__(input_size, output_size)\n",
    "    \n",
    "    def activation(self, input_data):\n",
    "        exp_input = np.exp(input_data - np.max(input_data, axis=1, keepdims=True))  # Numerical stability\n",
    "        return exp_input / np.sum(exp_input, axis=1, keepdims=True)\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate=0.1):\n",
    "\n",
    "        grad_input = grad_output * self.output * (1 - self.output)\n",
    "\n",
    "        grad_w = np.dot(self.input.T, grad_input)\n",
    "\n",
    "        self.weights -= learning_rate * grad_w\n",
    "\n",
    "        weight_with_bias = self.weights[1:, :]\n",
    "        \n",
    "        return np.dot(grad_input, weight_with_bias.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, layers) -> None:\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad , learning_rate=0.1):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad, learning_rate)\n",
    "        return grad\n",
    "\n",
    "    def cross_entropy_loss(self, output, target):\n",
    "        # epsilon = 1e-15  # Avoid log(0)\n",
    "        # output = np.clip(output, epsilon, 1 - epsilon)\n",
    "        return -np.sum(target * np.log(output)) / len(output)\n",
    "\n",
    "    def grad_cross_entropy_loss(self, output, target):\n",
    "        # epsilon = 1e-15\n",
    "        # output = np.clip(output, epsilon, 1 - epsilon)\n",
    "        return (output - target) # Len output divide check\n",
    "\n",
    "    def train(self, X_train, y_train, learning_rate=0.1, epochs=100, batch_size=32, min_delta=1e-6, patience=5):\n",
    "        \n",
    "        prev_loss = float('inf')\n",
    "        consecutive_no_improvement = 0\n",
    "           \n",
    "        num_samples = X_train.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_train_shuffled = X_train[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "\n",
    "            \n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                batch_X = X_train_shuffled[i:i + batch_size]\n",
    "                batch_y = y_train_shuffled[i:i + batch_size]\n",
    "                \n",
    "                output = self.forward(batch_X)\n",
    "                grad = self.grad_cross_entropy_loss(output, batch_y)\n",
    "\n",
    "                self.backward(grad, learning_rate)\n",
    "                \n",
    "            output = self.forward(X_train)\n",
    "            total_loss = self.cross_entropy_loss(output, y_train)\n",
    "            \n",
    "            if total_loss > prev_loss - min_delta:\n",
    "                consecutive_no_improvement += 1\n",
    "                if consecutive_no_improvement >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            else:\n",
    "                consecutive_no_improvement = 0\n",
    "            \n",
    "\n",
    "            print(f\"Epoch {epoch}: Loss {total_loss}\")\n",
    "                \n",
    "            \n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_path = 'part_b/x_train.npy'\n",
    "y_train_path = 'part_b/y_train.npy'\n",
    "\n",
    "X_train, y_train = get_data(x_train_path, y_train_path)\n",
    "\n",
    "x_test_path = 'part_b/x_test.npy'\n",
    "y_test_path = 'part_b/y_test.npy'\n",
    "\n",
    "X_test, y_test = get_data(x_test_path, y_test_path)\n",
    "\n",
    "#you might need one hot encoded y in part a,b,c,d,e\n",
    "label_encoder = OneHotEncoder(sparse_output = False)\n",
    "label_encoder.fit(np.expand_dims(y_train, axis = -1))\n",
    "\n",
    "y_train_onehot = label_encoder.transform(np.expand_dims(y_train, axis = -1))\n",
    "y_test_onehot = label_encoder.transform(np.expand_dims(y_test, axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = 10\n",
    "\n",
    "nn = NN([\n",
    "    SigmoidLayer(1024, hidden_layer),\n",
    "    SoftMaxLayer(hidden_layer, 5)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.8164999782867046\n",
      "Epoch 1: Loss 0.8199688514671065\n",
      "Epoch 2: Loss 0.8185295632853021\n",
      "Epoch 3: Loss 0.8141829784499667\n",
      "Epoch 4: Loss 0.8113387291782533\n",
      "Epoch 5: Loss 0.8150320483114504\n",
      "Epoch 6: Loss 0.8124899869146512\n",
      "Epoch 7: Loss 0.8099637572266917\n",
      "Epoch 8: Loss 0.8115363586482408\n",
      "Epoch 9: Loss 0.8108345760516494\n",
      "Epoch 10: Loss 0.8108690470553093\n",
      "Epoch 11: Loss 0.8128037160346134\n",
      "Epoch 12: Loss 0.8109155842409423\n",
      "Epoch 13: Loss 0.8076035896488382\n",
      "Epoch 14: Loss 0.810917335003658\n",
      "Epoch 15: Loss 0.8079765954011714\n",
      "Epoch 16: Loss 0.8106300455123141\n",
      "Epoch 17: Loss 0.808188084704589\n",
      "Epoch 18: Loss 0.8094029982724131\n",
      "Epoch 19: Loss 0.8084754068356308\n",
      "Epoch 20: Loss 0.807226514320198\n",
      "Epoch 21: Loss 0.8048640518250472\n",
      "Epoch 22: Loss 0.8078937017699027\n",
      "Epoch 23: Loss 0.8078858207090476\n",
      "Epoch 24: Loss 0.8089904154360367\n",
      "Epoch 25: Loss 0.8082901327295424\n",
      "Epoch 26: Loss 0.8044068727231104\n",
      "Epoch 27: Loss 0.8045665749511398\n",
      "Epoch 28: Loss 0.8068687347087483\n",
      "Epoch 29: Loss 0.8072810664681209\n",
      "Epoch 30: Loss 0.8083794694172445\n",
      "Epoch 31: Loss 0.804866951122322\n",
      "Epoch 32: Loss 0.8043135170207193\n",
      "Epoch 33: Loss 0.8039871364076545\n",
      "Epoch 34: Loss 0.8071231577135025\n",
      "Epoch 35: Loss 0.8022183077664018\n",
      "Epoch 36: Loss 0.800575696414394\n",
      "Epoch 37: Loss 0.8057873345118045\n",
      "Epoch 38: Loss 0.8042292266697235\n",
      "Epoch 39: Loss 0.799070717417595\n",
      "Epoch 40: Loss 0.8006168305210679\n",
      "Epoch 41: Loss 0.8034816422056228\n",
      "Epoch 42: Loss 0.7972482156072344\n",
      "Epoch 43: Loss 0.7992985470341334\n",
      "Epoch 44: Loss 0.8018350191508271\n",
      "Epoch 45: Loss 0.7991414720835455\n",
      "Epoch 46: Loss 0.7987916765940348\n",
      "Epoch 47: Loss 0.8016517909685822\n",
      "Epoch 48: Loss 0.7992581187856562\n",
      "Epoch 49: Loss 0.8005034400948945\n",
      "Epoch 50: Loss 0.8010169776344039\n",
      "Epoch 51: Loss 0.7962930117741626\n",
      "Epoch 52: Loss 0.796877156433293\n",
      "Epoch 53: Loss 0.8028990233473386\n",
      "Epoch 54: Loss 0.7961328044773222\n",
      "Epoch 55: Loss 0.7997055906075866\n",
      "Epoch 56: Loss 0.7971332614156934\n",
      "Epoch 57: Loss 0.793358893602727\n",
      "Epoch 58: Loss 0.8005277870923406\n",
      "Epoch 59: Loss 0.7970002781327578\n",
      "Epoch 60: Loss 0.8020745945679558\n",
      "Epoch 61: Loss 0.7990855475569401\n",
      "Epoch 62: Loss 0.7932618870621246\n",
      "Epoch 63: Loss 0.797693466434352\n",
      "Epoch 64: Loss 0.7940914815923302\n",
      "Epoch 65: Loss 0.7960257536015255\n",
      "Epoch 66: Loss 0.792404916886079\n",
      "Epoch 67: Loss 0.7951525058501605\n",
      "Epoch 68: Loss 0.7950935645342221\n",
      "Epoch 69: Loss 0.7963271835272677\n",
      "Epoch 70: Loss 0.7907735600891912\n",
      "Epoch 71: Loss 0.792156340448633\n",
      "Epoch 72: Loss 0.7895800622441871\n",
      "Epoch 73: Loss 0.7907905484610351\n",
      "Epoch 74: Loss 0.7883719890257296\n",
      "Epoch 75: Loss 0.7927893306405864\n",
      "Epoch 76: Loss 0.7917894547295122\n",
      "Epoch 77: Loss 0.7940121743299348\n",
      "Epoch 78: Loss 0.7878152870000691\n",
      "Epoch 79: Loss 0.7955895112134727\n",
      "Epoch 80: Loss 0.7910640679115906\n",
      "Epoch 81: Loss 0.7886547088512924\n",
      "Epoch 82: Loss 0.7937100919816873\n",
      "Epoch 83: Loss 0.7876896276562253\n",
      "Epoch 84: Loss 0.7861524097201387\n",
      "Epoch 85: Loss 0.7898494981200942\n",
      "Epoch 86: Loss 0.7873589067486032\n",
      "Epoch 87: Loss 0.7965593289023913\n",
      "Epoch 88: Loss 0.7888935870010951\n",
      "Epoch 89: Loss 0.789189636182983\n",
      "Epoch 90: Loss 0.7917899390718833\n",
      "Epoch 91: Loss 0.7904873987629633\n",
      "Epoch 92: Loss 0.7819968561098865\n",
      "Epoch 93: Loss 0.7919683859146924\n",
      "Epoch 94: Loss 0.7907945617957488\n",
      "Epoch 95: Loss 0.7847834242092416\n",
      "Epoch 96: Loss 0.7885842834620754\n",
      "Epoch 97: Loss 0.7852172403785722\n",
      "Epoch 98: Loss 0.7879146014895131\n",
      "Epoch 99: Loss 0.7892332890689224\n",
      "Epoch 100: Loss 0.786336040900421\n",
      "Epoch 101: Loss 0.7805289187978282\n",
      "Epoch 102: Loss 0.782303421029912\n",
      "Epoch 103: Loss 0.781174297239453\n",
      "Epoch 104: Loss 0.7849395623142686\n",
      "Epoch 105: Loss 0.7833532111780922\n",
      "Epoch 106: Loss 0.7869247985881631\n",
      "Epoch 107: Loss 0.7837396629222998\n",
      "Epoch 108: Loss 0.7807142100605928\n",
      "Epoch 109: Loss 0.7820687146764339\n",
      "Epoch 110: Loss 0.7884424723306152\n",
      "Epoch 111: Loss 0.7830462282866418\n",
      "Epoch 112: Loss 0.7836023704083763\n",
      "Epoch 113: Loss 0.7815566647940932\n",
      "Epoch 114: Loss 0.7799069236935859\n",
      "Epoch 115: Loss 0.7800666642366866\n",
      "Epoch 116: Loss 0.7787705780702021\n",
      "Epoch 117: Loss 0.7820272256902013\n",
      "Epoch 118: Loss 0.7785734439217095\n",
      "Epoch 119: Loss 0.7798400097211674\n",
      "Epoch 120: Loss 0.7796584574135491\n",
      "Epoch 121: Loss 0.7802200226867635\n",
      "Epoch 122: Loss 0.7812964586165577\n",
      "Epoch 123: Loss 0.7797472830157943\n",
      "Epoch 124: Loss 0.7829039470131266\n",
      "Epoch 125: Loss 0.7777084979910147\n",
      "Epoch 126: Loss 0.7782314525771425\n",
      "Epoch 127: Loss 0.7757578201616893\n",
      "Epoch 128: Loss 0.7799475012402031\n",
      "Epoch 129: Loss 0.7802402088868429\n",
      "Epoch 130: Loss 0.776843125263864\n",
      "Epoch 131: Loss 0.7771931701248652\n",
      "Epoch 132: Loss 0.7769172389645062\n",
      "Epoch 133: Loss 0.7780712659963738\n",
      "Epoch 134: Loss 0.7763992018289243\n",
      "Epoch 135: Loss 0.7752718693156656\n",
      "Epoch 136: Loss 0.7820832876811322\n",
      "Epoch 137: Loss 0.7749571817149641\n",
      "Epoch 138: Loss 0.7754007310442383\n",
      "Epoch 139: Loss 0.7763584777613387\n",
      "Epoch 140: Loss 0.7761097599546424\n",
      "Epoch 141: Loss 0.7731319165137447\n",
      "Epoch 142: Loss 0.7813803969333819\n",
      "Epoch 143: Loss 0.7752397577142645\n",
      "Epoch 144: Loss 0.7754598266876777\n",
      "Epoch 145: Loss 0.771556467866403\n",
      "Epoch 146: Loss 0.7726294334318518\n",
      "Epoch 147: Loss 0.7748395043614893\n",
      "Epoch 148: Loss 0.7797530985427209\n",
      "Epoch 149: Loss 0.7761786952768323\n",
      "Epoch 150: Loss 0.7759698096784217\n",
      "Epoch 151: Loss 0.7724919823732235\n",
      "Epoch 152: Loss 0.777259748405811\n",
      "Epoch 153: Loss 0.7735745896584881\n",
      "Epoch 154: Loss 0.7751138147184286\n",
      "Epoch 155: Loss 0.7760028267355724\n",
      "Epoch 156: Loss 0.7766793412810508\n",
      "Epoch 157: Loss 0.7742569490797899\n",
      "Epoch 158: Loss 0.77499278924702\n",
      "Epoch 159: Loss 0.7731613722838956\n",
      "Epoch 160: Loss 0.7716345388240519\n",
      "Epoch 161: Loss 0.7699820586684184\n",
      "Epoch 162: Loss 0.7722331381757381\n",
      "Epoch 163: Loss 0.7713197046179306\n",
      "Epoch 164: Loss 0.7752267725226787\n",
      "Epoch 165: Loss 0.7718172118872503\n",
      "Epoch 166: Loss 0.7693439911343152\n",
      "Epoch 167: Loss 0.7671339866606488\n",
      "Epoch 168: Loss 0.7713161452938466\n",
      "Epoch 169: Loss 0.7705090511515847\n",
      "Epoch 170: Loss 0.7687205564283086\n",
      "Epoch 171: Loss 0.7676239092131364\n",
      "Epoch 172: Loss 0.7723817994766029\n",
      "Epoch 173: Loss 0.7700254642782847\n",
      "Epoch 174: Loss 0.771048228593693\n",
      "Epoch 175: Loss 0.7690908693125847\n",
      "Epoch 176: Loss 0.7721788594051389\n",
      "Epoch 177: Loss 0.7703415084847804\n",
      "Epoch 178: Loss 0.7682797164051602\n",
      "Epoch 179: Loss 0.7730476896860586\n",
      "Epoch 180: Loss 0.7664024020418884\n",
      "Epoch 181: Loss 0.7676683905333678\n",
      "Epoch 182: Loss 0.7699279786739325\n",
      "Epoch 183: Loss 0.7683430141569632\n",
      "Epoch 184: Loss 0.7663812049302012\n",
      "Epoch 185: Loss 0.7654092725144286\n",
      "Epoch 186: Loss 0.7757098932148947\n",
      "Epoch 187: Loss 0.7642357625908451\n",
      "Epoch 188: Loss 0.7707348861029854\n",
      "Epoch 189: Loss 0.7645851966375821\n",
      "Epoch 190: Loss 0.7676967482902094\n",
      "Epoch 191: Loss 0.7648573599509774\n",
      "Epoch 192: Loss 0.7647739776060766\n",
      "Epoch 193: Loss 0.7658760382227476\n",
      "Epoch 194: Loss 0.7655284269023656\n",
      "Epoch 195: Loss 0.7634190204181271\n",
      "Epoch 196: Loss 0.7659599048607635\n",
      "Epoch 197: Loss 0.7655513621964546\n",
      "Epoch 198: Loss 0.7641015971791665\n",
      "Epoch 199: Loss 0.766287715743786\n",
      "Epoch 200: Loss 0.7655629610628057\n",
      "Epoch 201: Loss 0.7672925176692217\n",
      "Epoch 202: Loss 0.766854551247387\n",
      "Epoch 203: Loss 0.7636619050712954\n",
      "Epoch 204: Loss 0.7635558946375377\n",
      "Epoch 205: Loss 0.764567625667307\n",
      "Epoch 206: Loss 0.7620973485306954\n",
      "Epoch 207: Loss 0.7681726381984997\n",
      "Epoch 208: Loss 0.7649641274033211\n",
      "Epoch 209: Loss 0.7615930954755984\n",
      "Epoch 210: Loss 0.7631086016433124\n",
      "Epoch 211: Loss 0.7662072418072589\n",
      "Epoch 212: Loss 0.7615342303713099\n",
      "Epoch 213: Loss 0.7616177318758259\n",
      "Epoch 214: Loss 0.7634034141801272\n",
      "Epoch 215: Loss 0.7602500404903176\n",
      "Epoch 216: Loss 0.7609530570744775\n",
      "Epoch 217: Loss 0.7652155006027627\n",
      "Epoch 218: Loss 0.7636142734998647\n",
      "Epoch 219: Loss 0.7569220884497156\n",
      "Epoch 220: Loss 0.7616948769097538\n",
      "Epoch 221: Loss 0.763244265775251\n",
      "Epoch 222: Loss 0.7640668871959487\n",
      "Epoch 223: Loss 0.7600716464867728\n",
      "Epoch 224: Loss 0.7673703661379235\n",
      "Epoch 225: Loss 0.7598354218795099\n",
      "Epoch 226: Loss 0.759994863450293\n",
      "Epoch 227: Loss 0.7611236554572245\n",
      "Epoch 228: Loss 0.7616965572310357\n",
      "Epoch 229: Loss 0.7638150487051862\n",
      "Epoch 230: Loss 0.7595726220120093\n",
      "Epoch 231: Loss 0.7587670464621273\n",
      "Epoch 232: Loss 0.7613644444282117\n",
      "Epoch 233: Loss 0.7603670639121523\n",
      "Epoch 234: Loss 0.7641474179182876\n",
      "Epoch 235: Loss 0.7629310396311916\n",
      "Epoch 236: Loss 0.7590260181121424\n",
      "Epoch 237: Loss 0.7581393646967917\n",
      "Epoch 238: Loss 0.7591935794975806\n",
      "Epoch 239: Loss 0.7627123834742181\n",
      "Epoch 240: Loss 0.756659439168671\n",
      "Epoch 241: Loss 0.7595070021051086\n",
      "Epoch 242: Loss 0.7576733553968785\n",
      "Epoch 243: Loss 0.7644456806464316\n",
      "Epoch 244: Loss 0.75558083040957\n",
      "Epoch 245: Loss 0.7568160278837396\n",
      "Epoch 246: Loss 0.7594860833829724\n",
      "Epoch 247: Loss 0.7560852930941421\n",
      "Epoch 248: Loss 0.7548644970120738\n",
      "Epoch 249: Loss 0.7562047054504849\n",
      "Epoch 250: Loss 0.7552542648791803\n",
      "Epoch 251: Loss 0.7560632233992631\n",
      "Epoch 252: Loss 0.7597345117288298\n",
      "Epoch 253: Loss 0.7551548796643968\n",
      "Epoch 254: Loss 0.7582987591370118\n",
      "Epoch 255: Loss 0.7598825698225684\n",
      "Epoch 256: Loss 0.7561209081356104\n",
      "Epoch 257: Loss 0.7549042112092097\n",
      "Epoch 258: Loss 0.7531956685236061\n",
      "Epoch 259: Loss 0.757549407644655\n",
      "Epoch 260: Loss 0.7573463348974414\n",
      "Epoch 261: Loss 0.7574240713043048\n",
      "Epoch 262: Loss 0.7544074937903112\n",
      "Epoch 263: Loss 0.7546200440744352\n",
      "Epoch 264: Loss 0.7555592900126236\n",
      "Epoch 265: Loss 0.7546030581017726\n",
      "Epoch 266: Loss 0.7534847395369271\n",
      "Epoch 267: Loss 0.7582702863779175\n",
      "Epoch 268: Loss 0.7532548155917956\n",
      "Epoch 269: Loss 0.7543664974759005\n",
      "Epoch 270: Loss 0.7535059263176617\n",
      "Epoch 271: Loss 0.7524997161975422\n",
      "Epoch 272: Loss 0.7535470631799442\n",
      "Epoch 273: Loss 0.7567984433893697\n",
      "Epoch 274: Loss 0.7536945573291783\n",
      "Epoch 275: Loss 0.7558627414692057\n",
      "Epoch 276: Loss 0.7558904848241811\n",
      "Epoch 277: Loss 0.7526103591493136\n",
      "Epoch 278: Loss 0.7520858070116088\n",
      "Epoch 279: Loss 0.7540984426752393\n",
      "Epoch 280: Loss 0.7519253797814099\n",
      "Epoch 281: Loss 0.7551087935932358\n",
      "Epoch 282: Loss 0.7514644486002964\n",
      "Epoch 283: Loss 0.7516205178019247\n",
      "Epoch 284: Loss 0.7539231341068051\n",
      "Epoch 285: Loss 0.7553317637514367\n",
      "Epoch 286: Loss 0.7507028601997081\n",
      "Epoch 287: Loss 0.7526612076598033\n",
      "Epoch 288: Loss 0.7496288506282923\n",
      "Epoch 289: Loss 0.7553768160187653\n",
      "Epoch 290: Loss 0.7556362900387743\n",
      "Epoch 291: Loss 0.7490680299862227\n",
      "Epoch 292: Loss 0.7524108133856989\n",
      "Epoch 293: Loss 0.7527611913674392\n",
      "Epoch 294: Loss 0.7510326628111718\n",
      "Epoch 295: Loss 0.749903952359846\n",
      "Epoch 296: Loss 0.7509825244416095\n",
      "Epoch 297: Loss 0.7515323109923608\n",
      "Epoch 298: Loss 0.7522015563701985\n",
      "Epoch 299: Loss 0.7494366793606267\n",
      "Epoch 300: Loss 0.7476629548810287\n",
      "Epoch 301: Loss 0.7479252477243793\n",
      "Epoch 302: Loss 0.7477617160549234\n",
      "Epoch 303: Loss 0.7485694793167238\n",
      "Epoch 304: Loss 0.7494999918971289\n",
      "Epoch 305: Loss 0.7495552687502008\n",
      "Epoch 306: Loss 0.7493539352078368\n",
      "Epoch 307: Loss 0.7474816070189867\n",
      "Epoch 308: Loss 0.7519552611549717\n",
      "Epoch 309: Loss 0.749363530400538\n",
      "Epoch 310: Loss 0.7450405949170232\n",
      "Epoch 311: Loss 0.7502437256501193\n",
      "Epoch 312: Loss 0.7469004894525324\n",
      "Epoch 313: Loss 0.7493769326173388\n",
      "Epoch 314: Loss 0.7444174093014989\n",
      "Epoch 315: Loss 0.751744307765439\n",
      "Epoch 316: Loss 0.7500724001384076\n",
      "Epoch 317: Loss 0.7476341592947431\n",
      "Epoch 318: Loss 0.7453536896611871\n",
      "Epoch 319: Loss 0.7493549326812238\n",
      "Epoch 320: Loss 0.7495405948377188\n",
      "Epoch 321: Loss 0.7518970436694876\n",
      "Epoch 322: Loss 0.7468328288108742\n",
      "Epoch 323: Loss 0.7441523590904191\n",
      "Epoch 324: Loss 0.7463247813910446\n",
      "Epoch 325: Loss 0.7509847902505452\n",
      "Epoch 326: Loss 0.7463731805481624\n",
      "Epoch 327: Loss 0.7461344789994842\n",
      "Epoch 328: Loss 0.7449200144671003\n",
      "Epoch 329: Loss 0.7449295580496574\n",
      "Epoch 330: Loss 0.7482188603897048\n",
      "Epoch 331: Loss 0.7472610042961192\n",
      "Epoch 332: Loss 0.7446004859452547\n",
      "Epoch 333: Loss 0.7510177844171898\n",
      "Epoch 334: Loss 0.7458408681717197\n",
      "Epoch 335: Loss 0.7454156291948836\n",
      "Epoch 336: Loss 0.7441722107054911\n",
      "Epoch 337: Loss 0.7453338809000363\n",
      "Epoch 338: Loss 0.743661580358338\n",
      "Epoch 339: Loss 0.7495215188744628\n",
      "Epoch 340: Loss 0.7459652262034913\n",
      "Epoch 341: Loss 0.7425812671062353\n",
      "Epoch 342: Loss 0.7441750470106354\n",
      "Epoch 343: Loss 0.7439168385624653\n",
      "Epoch 344: Loss 0.7457114253610283\n",
      "Epoch 345: Loss 0.741924837379521\n",
      "Epoch 346: Loss 0.7451239733612675\n",
      "Epoch 347: Loss 0.7455959941963404\n",
      "Epoch 348: Loss 0.7452931168254416\n",
      "Epoch 349: Loss 0.7436631942257117\n",
      "Epoch 350: Loss 0.7426157533156156\n",
      "Epoch 351: Loss 0.743774860002933\n",
      "Epoch 352: Loss 0.744136865961919\n",
      "Epoch 353: Loss 0.7425891535556929\n",
      "Epoch 354: Loss 0.7422631645367316\n",
      "Epoch 355: Loss 0.7448971000967679\n",
      "Epoch 356: Loss 0.7446029775923801\n",
      "Epoch 357: Loss 0.7401531033025348\n",
      "Epoch 358: Loss 0.7419884410139382\n",
      "Epoch 359: Loss 0.7419799071263921\n",
      "Epoch 360: Loss 0.7415176188815208\n",
      "Epoch 361: Loss 0.7425630201788431\n",
      "Epoch 362: Loss 0.7434889520453853\n",
      "Epoch 363: Loss 0.7405062564403946\n",
      "Epoch 364: Loss 0.7449588650991994\n",
      "Epoch 365: Loss 0.7425469913220494\n",
      "Epoch 366: Loss 0.7412601542103884\n",
      "Epoch 367: Loss 0.7432595151313902\n",
      "Epoch 368: Loss 0.7443808907465441\n",
      "Epoch 369: Loss 0.7419027915346558\n",
      "Epoch 370: Loss 0.7398111123931832\n",
      "Epoch 371: Loss 0.7435189136594429\n",
      "Epoch 372: Loss 0.7380996194001207\n",
      "Epoch 373: Loss 0.7398654100568981\n",
      "Epoch 374: Loss 0.7389010124988188\n",
      "Epoch 375: Loss 0.7383928125760312\n",
      "Epoch 376: Loss 0.7449900518551047\n",
      "Epoch 377: Loss 0.7421446459039471\n",
      "Epoch 378: Loss 0.7399043345425965\n",
      "Epoch 379: Loss 0.7402462886138454\n",
      "Epoch 380: Loss 0.740617763136553\n",
      "Epoch 381: Loss 0.7387202035544036\n",
      "Epoch 382: Loss 0.7412045582802012\n",
      "Epoch 383: Loss 0.737795179261754\n",
      "Epoch 384: Loss 0.7430849403085157\n",
      "Epoch 385: Loss 0.7445665988963607\n",
      "Epoch 386: Loss 0.7463897329645159\n",
      "Epoch 387: Loss 0.7415235153668062\n",
      "Epoch 388: Loss 0.7380430143937042\n",
      "Epoch 389: Loss 0.7395851385355119\n",
      "Epoch 390: Loss 0.7392781068674242\n",
      "Epoch 391: Loss 0.7409124063553681\n",
      "Epoch 392: Loss 0.7394089837677159\n",
      "Epoch 393: Loss 0.7404245788314371\n",
      "Epoch 394: Loss 0.7393055287924077\n",
      "Epoch 395: Loss 0.7380125046746764\n",
      "Epoch 396: Loss 0.7364700434746085\n",
      "Epoch 397: Loss 0.7376045019075166\n",
      "Epoch 398: Loss 0.7387082863392328\n",
      "Epoch 399: Loss 0.7392572283559251\n",
      "Epoch 400: Loss 0.7394965855621468\n",
      "Epoch 401: Loss 0.7400159404333444\n",
      "Epoch 402: Loss 0.7377795401197148\n",
      "Epoch 403: Loss 0.7390976806266264\n",
      "Epoch 404: Loss 0.7399177742941386\n",
      "Epoch 405: Loss 0.7384262735756292\n",
      "Epoch 406: Loss 0.7440089332802435\n",
      "Epoch 407: Loss 0.7355015355544481\n",
      "Epoch 408: Loss 0.7375875715687794\n",
      "Epoch 409: Loss 0.7391739182376525\n",
      "Epoch 410: Loss 0.7371247833034003\n",
      "Epoch 411: Loss 0.737990084953258\n",
      "Epoch 412: Loss 0.7360438358940881\n",
      "Epoch 413: Loss 0.7382373326431654\n",
      "Epoch 414: Loss 0.734715026575725\n",
      "Epoch 415: Loss 0.7368030848561175\n",
      "Epoch 416: Loss 0.7346377626443231\n",
      "Epoch 417: Loss 0.7373658273167671\n",
      "Epoch 418: Loss 0.7406188393527808\n",
      "Epoch 419: Loss 0.7370583840688004\n",
      "Epoch 420: Loss 0.7347412100508359\n",
      "Epoch 421: Loss 0.7367061730520968\n",
      "Epoch 422: Loss 0.7349443984650589\n",
      "Epoch 423: Loss 0.734525421674747\n",
      "Epoch 424: Loss 0.7375516545119006\n",
      "Epoch 425: Loss 0.73732022385154\n",
      "Epoch 426: Loss 0.7366237691716049\n",
      "Epoch 427: Loss 0.7372992532362076\n",
      "Epoch 428: Loss 0.7338958338761937\n",
      "Epoch 429: Loss 0.7354683090734536\n",
      "Epoch 430: Loss 0.7399879541021748\n",
      "Epoch 431: Loss 0.733531864439229\n",
      "Epoch 432: Loss 0.7393483642860125\n",
      "Epoch 433: Loss 0.7345159950617234\n",
      "Epoch 434: Loss 0.738275236236977\n",
      "Epoch 435: Loss 0.7405043618001985\n",
      "Epoch 436: Loss 0.7362333233309195\n",
      "Epoch 437: Loss 0.7351898402865275\n",
      "Epoch 438: Loss 0.7360551176764374\n",
      "Epoch 439: Loss 0.7352280331746284\n",
      "Epoch 440: Loss 0.7362586021220094\n",
      "Epoch 441: Loss 0.7351285729069821\n",
      "Epoch 442: Loss 0.7353441599888927\n",
      "Epoch 443: Loss 0.7336625826826049\n",
      "Epoch 444: Loss 0.7326864556983166\n",
      "Epoch 445: Loss 0.7335349568053963\n",
      "Epoch 446: Loss 0.7328534728883997\n",
      "Epoch 447: Loss 0.7322759987579273\n",
      "Epoch 448: Loss 0.733653673460161\n",
      "Epoch 449: Loss 0.7333202515301528\n",
      "Epoch 450: Loss 0.7332853232615822\n",
      "Epoch 451: Loss 0.7377764113555928\n",
      "Epoch 452: Loss 0.7343763280473762\n",
      "Epoch 453: Loss 0.7346138399138622\n",
      "Epoch 454: Loss 0.7362996014438354\n",
      "Epoch 455: Loss 0.7322512055243875\n",
      "Epoch 456: Loss 0.733611885737341\n",
      "Epoch 457: Loss 0.7341996068378546\n",
      "Epoch 458: Loss 0.7330111151163382\n",
      "Epoch 459: Loss 0.7330302044357668\n",
      "Epoch 460: Loss 0.7381414484434194\n",
      "Epoch 461: Loss 0.7346238691103779\n",
      "Epoch 462: Loss 0.7349627016075868\n",
      "Epoch 463: Loss 0.7315135731276344\n",
      "Epoch 464: Loss 0.7316840438206847\n",
      "Epoch 465: Loss 0.7324098483720404\n",
      "Epoch 466: Loss 0.731821866054597\n",
      "Epoch 467: Loss 0.7338739455671446\n",
      "Epoch 468: Loss 0.7306724468641194\n",
      "Epoch 469: Loss 0.7320118010460306\n",
      "Epoch 470: Loss 0.7318535262640942\n",
      "Epoch 471: Loss 0.7318080520286022\n",
      "Epoch 472: Loss 0.7342015831010271\n",
      "Epoch 473: Loss 0.732290500501416\n",
      "Epoch 474: Loss 0.7312569083488846\n",
      "Epoch 475: Loss 0.7362065859634027\n",
      "Epoch 476: Loss 0.735174115132701\n",
      "Epoch 477: Loss 0.7309205911214834\n",
      "Epoch 478: Loss 0.7313411983750426\n",
      "Epoch 479: Loss 0.7320865329768274\n",
      "Epoch 480: Loss 0.7316328172761157\n",
      "Epoch 481: Loss 0.730890608300382\n",
      "Epoch 482: Loss 0.730579074910425\n",
      "Epoch 483: Loss 0.730153726842085\n",
      "Epoch 484: Loss 0.7302902984327215\n",
      "Epoch 485: Loss 0.7304185175127533\n",
      "Epoch 486: Loss 0.7336914598840502\n",
      "Epoch 487: Loss 0.7291809042630006\n",
      "Epoch 488: Loss 0.7292906811016433\n",
      "Epoch 489: Loss 0.7290588315124865\n",
      "Epoch 490: Loss 0.7300236045409622\n",
      "Epoch 491: Loss 0.7293207299036447\n",
      "Epoch 492: Loss 0.7346094760589299\n",
      "Epoch 493: Loss 0.7319180290713728\n",
      "Epoch 494: Loss 0.7303411970509971\n",
      "Epoch 495: Loss 0.7287686874427213\n",
      "Epoch 496: Loss 0.7295377436558013\n",
      "Epoch 497: Loss 0.7319623233178629\n",
      "Epoch 498: Loss 0.7311333087174242\n",
      "Epoch 499: Loss 0.7274372626851074\n",
      "Epoch 500: Loss 0.7342309836967648\n",
      "Epoch 501: Loss 0.730150883861667\n",
      "Epoch 502: Loss 0.7321174475410165\n",
      "Epoch 503: Loss 0.7284910121372653\n",
      "Epoch 504: Loss 0.7376324621147904\n",
      "Epoch 505: Loss 0.727275059759562\n",
      "Epoch 506: Loss 0.7284104084010024\n",
      "Epoch 507: Loss 0.7274026774310504\n",
      "Epoch 508: Loss 0.7277519986997786\n",
      "Epoch 509: Loss 0.7302492845040307\n",
      "Epoch 510: Loss 0.7309683188764963\n",
      "Epoch 511: Loss 0.7274164351767832\n",
      "Epoch 512: Loss 0.7280192740686405\n",
      "Epoch 513: Loss 0.7265044667624606\n",
      "Epoch 514: Loss 0.7261777683351099\n",
      "Epoch 515: Loss 0.7280068482599681\n",
      "Epoch 516: Loss 0.7279379096318306\n",
      "Epoch 517: Loss 0.733146861986831\n",
      "Epoch 518: Loss 0.728997069607848\n",
      "Epoch 519: Loss 0.7286725680541946\n",
      "Epoch 520: Loss 0.7263978895233484\n",
      "Epoch 521: Loss 0.7272010813336715\n",
      "Epoch 522: Loss 0.7253053381048615\n",
      "Epoch 523: Loss 0.7254766454008313\n",
      "Epoch 524: Loss 0.7269422940903192\n",
      "Epoch 525: Loss 0.725392239047855\n",
      "Epoch 526: Loss 0.7281280455761134\n",
      "Epoch 527: Loss 0.7260045354535314\n",
      "Epoch 528: Loss 0.72665704038989\n",
      "Epoch 529: Loss 0.7258207254685404\n",
      "Epoch 530: Loss 0.7290032582090796\n",
      "Epoch 531: Loss 0.7243676403055971\n",
      "Epoch 532: Loss 0.7289569036555066\n",
      "Epoch 533: Loss 0.7278299377916685\n",
      "Epoch 534: Loss 0.7258619878324338\n",
      "Epoch 535: Loss 0.7268932068154794\n",
      "Epoch 536: Loss 0.72710683295724\n",
      "Epoch 537: Loss 0.7271787470525909\n",
      "Epoch 538: Loss 0.72831858861954\n",
      "Epoch 539: Loss 0.7271788063325902\n",
      "Epoch 540: Loss 0.730185157679376\n",
      "Epoch 541: Loss 0.7276449329079889\n",
      "Epoch 542: Loss 0.727772890686293\n",
      "Epoch 543: Loss 0.7279146401641552\n",
      "Epoch 544: Loss 0.7276130368745799\n",
      "Epoch 545: Loss 0.7235872959986104\n",
      "Epoch 546: Loss 0.731077273719021\n",
      "Epoch 547: Loss 0.7281171438888401\n",
      "Epoch 548: Loss 0.7274409769313929\n",
      "Epoch 549: Loss 0.7247157547669935\n",
      "Epoch 550: Loss 0.7274336068117322\n",
      "Epoch 551: Loss 0.7286698409048593\n",
      "Epoch 552: Loss 0.7262947455949872\n",
      "Epoch 553: Loss 0.7296364000261073\n",
      "Epoch 554: Loss 0.7233492183839009\n",
      "Epoch 555: Loss 0.7245905983121046\n",
      "Epoch 556: Loss 0.7263294638211811\n",
      "Epoch 557: Loss 0.7262275809736413\n",
      "Epoch 558: Loss 0.7258707641102486\n",
      "Epoch 559: Loss 0.7240460148332195\n",
      "Epoch 560: Loss 0.7258964094440616\n",
      "Epoch 561: Loss 0.723049603835922\n",
      "Epoch 562: Loss 0.7261540474063136\n",
      "Epoch 563: Loss 0.7260432625325569\n",
      "Epoch 564: Loss 0.7276522943246309\n",
      "Epoch 565: Loss 0.7239520405435811\n",
      "Epoch 566: Loss 0.7273592481218885\n",
      "Epoch 567: Loss 0.7278577065670898\n",
      "Epoch 568: Loss 0.7238226306967676\n",
      "Epoch 569: Loss 0.7238693970852188\n",
      "Epoch 570: Loss 0.7234728299289972\n",
      "Epoch 571: Loss 0.723612013798319\n",
      "Epoch 572: Loss 0.7244552368371375\n",
      "Epoch 573: Loss 0.725704000967781\n",
      "Epoch 574: Loss 0.7233645874614234\n",
      "Epoch 575: Loss 0.7262586891207704\n",
      "Epoch 576: Loss 0.724140985923414\n",
      "Epoch 577: Loss 0.7223746859487327\n",
      "Epoch 578: Loss 0.7229205984878908\n",
      "Epoch 579: Loss 0.725161450042414\n",
      "Epoch 580: Loss 0.7261592978798507\n",
      "Epoch 581: Loss 0.7262677678967727\n",
      "Epoch 582: Loss 0.7222317425928206\n",
      "Epoch 583: Loss 0.7225274724101591\n",
      "Epoch 584: Loss 0.722979615322621\n",
      "Epoch 585: Loss 0.7227025115898608\n",
      "Epoch 586: Loss 0.7219608434071599\n",
      "Epoch 587: Loss 0.7212345317132417\n",
      "Epoch 588: Loss 0.7220193872459967\n",
      "Epoch 589: Loss 0.7227612511773117\n",
      "Epoch 590: Loss 0.721495620022026\n",
      "Epoch 591: Loss 0.7211876509200571\n",
      "Epoch 592: Loss 0.7227201981734463\n",
      "Epoch 593: Loss 0.7227405149252111\n",
      "Epoch 594: Loss 0.7223750984976615\n",
      "Epoch 595: Loss 0.7245209324613323\n",
      "Epoch 596: Loss 0.7223787805275645\n",
      "Epoch 597: Loss 0.7250342479950347\n",
      "Epoch 598: Loss 0.7234674134897228\n",
      "Epoch 599: Loss 0.7224910801597129\n",
      "Epoch 600: Loss 0.7224060166345263\n",
      "Epoch 601: Loss 0.7198612424635467\n",
      "Epoch 602: Loss 0.722444501638171\n",
      "Epoch 603: Loss 0.7220623734254448\n",
      "Epoch 604: Loss 0.7214567440712223\n",
      "Epoch 605: Loss 0.7256495613340825\n",
      "Epoch 606: Loss 0.723259310080348\n",
      "Epoch 607: Loss 0.7216758989197105\n",
      "Epoch 608: Loss 0.723003479500545\n",
      "Epoch 609: Loss 0.7205470176062275\n",
      "Epoch 610: Loss 0.7205384483665477\n",
      "Epoch 611: Loss 0.7226239437384021\n",
      "Epoch 612: Loss 0.7243776650784671\n",
      "Epoch 613: Loss 0.7256781670918911\n",
      "Epoch 614: Loss 0.7257757676876944\n",
      "Epoch 615: Loss 0.7219256467532765\n",
      "Epoch 616: Loss 0.7216235157007562\n",
      "Epoch 617: Loss 0.7239490790030685\n",
      "Epoch 618: Loss 0.7235831990497491\n",
      "Epoch 619: Loss 0.7207162461036948\n",
      "Epoch 620: Loss 0.7190891133679441\n",
      "Epoch 621: Loss 0.7242438345613358\n",
      "Epoch 622: Loss 0.720570712483137\n",
      "Epoch 623: Loss 0.7237519084607772\n",
      "Epoch 624: Loss 0.7188913927017041\n",
      "Epoch 625: Loss 0.7197527816865781\n",
      "Epoch 626: Loss 0.7206229905593373\n",
      "Epoch 627: Loss 0.7213612330328859\n",
      "Epoch 628: Loss 0.7199397125765877\n",
      "Epoch 629: Loss 0.7224099309185854\n",
      "Epoch 630: Loss 0.7220696199893981\n",
      "Epoch 631: Loss 0.7207918846125203\n",
      "Epoch 632: Loss 0.7194021626199926\n",
      "Epoch 633: Loss 0.7197821450516912\n",
      "Epoch 634: Loss 0.7217496620114058\n",
      "Epoch 635: Loss 0.7220268525387858\n",
      "Epoch 636: Loss 0.7229841287032905\n",
      "Epoch 637: Loss 0.7188444881858526\n",
      "Epoch 638: Loss 0.7200373729651459\n",
      "Epoch 639: Loss 0.7206732578837703\n",
      "Epoch 640: Loss 0.7215138322271845\n",
      "Epoch 641: Loss 0.7221390041604049\n",
      "Epoch 642: Loss 0.72163463260938\n",
      "Epoch 643: Loss 0.7194071181298491\n",
      "Epoch 644: Loss 0.7198134896649842\n",
      "Epoch 645: Loss 0.7230061101398154\n",
      "Epoch 646: Loss 0.7181025160636221\n",
      "Epoch 647: Loss 0.7201481836159054\n",
      "Epoch 648: Loss 0.7191997314637815\n",
      "Epoch 649: Loss 0.721064407160581\n",
      "Epoch 650: Loss 0.7223422429836914\n",
      "Epoch 651: Loss 0.7204297405119638\n",
      "Epoch 652: Loss 0.7211719383318468\n",
      "Epoch 653: Loss 0.7190774025120686\n",
      "Epoch 654: Loss 0.7179473080112945\n",
      "Epoch 655: Loss 0.7177149901476547\n",
      "Epoch 656: Loss 0.7198565128368576\n",
      "Epoch 657: Loss 0.7216826965402388\n",
      "Epoch 658: Loss 0.7201152320601624\n",
      "Epoch 659: Loss 0.7178354881047081\n",
      "Epoch 660: Loss 0.720310889902881\n",
      "Epoch 661: Loss 0.7185452597426214\n",
      "Epoch 662: Loss 0.7174573933998878\n",
      "Epoch 663: Loss 0.7180767099219384\n",
      "Epoch 664: Loss 0.718191427758428\n",
      "Epoch 665: Loss 0.7192876308439464\n",
      "Epoch 666: Loss 0.7180385652830577\n",
      "Epoch 667: Loss 0.722254189202654\n",
      "Epoch 668: Loss 0.719323745707937\n",
      "Epoch 669: Loss 0.7173420015745366\n",
      "Epoch 670: Loss 0.720664749455341\n",
      "Epoch 671: Loss 0.7172843337974893\n",
      "Epoch 672: Loss 0.7179601421137377\n",
      "Epoch 673: Loss 0.7188100690350285\n",
      "Epoch 674: Loss 0.7181299687809329\n",
      "Epoch 675: Loss 0.718902634660272\n",
      "Epoch 676: Loss 0.7194574489314657\n",
      "Epoch 677: Loss 0.7197825591212652\n",
      "Epoch 678: Loss 0.7189575819709458\n",
      "Epoch 679: Loss 0.718403322542609\n",
      "Epoch 680: Loss 0.7172072657964929\n",
      "Epoch 681: Loss 0.7218786064791066\n",
      "Epoch 682: Loss 0.7172231520424902\n",
      "Epoch 683: Loss 0.7178450644717291\n",
      "Epoch 684: Loss 0.7183753595764066\n",
      "Epoch 685: Loss 0.719117488677539\n",
      "Epoch 686: Loss 0.717318412893813\n",
      "Epoch 687: Loss 0.718872646001054\n",
      "Epoch 688: Loss 0.7178990742341756\n",
      "Epoch 689: Loss 0.7180924071345673\n",
      "Epoch 690: Loss 0.7204914311774266\n",
      "Epoch 691: Loss 0.7169532512723088\n",
      "Epoch 692: Loss 0.7182619453363617\n",
      "Epoch 693: Loss 0.7158919381607071\n",
      "Epoch 694: Loss 0.718865303746467\n",
      "Epoch 695: Loss 0.718453164191247\n",
      "Epoch 696: Loss 0.7156415146694077\n",
      "Epoch 697: Loss 0.7154027827389188\n",
      "Epoch 698: Loss 0.7162183604989406\n",
      "Epoch 699: Loss 0.7194435740876929\n",
      "Epoch 700: Loss 0.7163026098960553\n",
      "Epoch 701: Loss 0.7168160375641737\n",
      "Epoch 702: Loss 0.7151420531570827\n",
      "Epoch 703: Loss 0.7168171392491088\n",
      "Epoch 704: Loss 0.7151882813710811\n",
      "Epoch 705: Loss 0.7169584254352654\n",
      "Epoch 706: Loss 0.7174938694997529\n",
      "Epoch 707: Loss 0.721285763160014\n",
      "Epoch 708: Loss 0.7177859914798304\n",
      "Epoch 709: Loss 0.7163460024092119\n",
      "Epoch 710: Loss 0.71457112593748\n",
      "Epoch 711: Loss 0.7148812188297985\n",
      "Epoch 712: Loss 0.7152784080824808\n",
      "Epoch 713: Loss 0.714990140773468\n",
      "Epoch 714: Loss 0.7162693399654195\n",
      "Epoch 715: Loss 0.7152235401684452\n",
      "Epoch 716: Loss 0.7152944963415182\n",
      "Epoch 717: Loss 0.7147274739114315\n",
      "Epoch 718: Loss 0.7160459327519171\n",
      "Epoch 719: Loss 0.7201934666880256\n",
      "Epoch 720: Loss 0.717101289089126\n",
      "Epoch 721: Loss 0.7166506005394994\n",
      "Epoch 722: Loss 0.7157888404373651\n",
      "Epoch 723: Loss 0.7149223085775299\n",
      "Epoch 724: Loss 0.7179148355913197\n",
      "Epoch 725: Loss 0.7148453223689472\n",
      "Epoch 726: Loss 0.7159048481740936\n",
      "Epoch 727: Loss 0.7139562117522322\n",
      "Epoch 728: Loss 0.7183946807872438\n",
      "Epoch 729: Loss 0.7230385676008133\n",
      "Epoch 730: Loss 0.715298062733714\n",
      "Epoch 731: Loss 0.7138269969666295\n",
      "Epoch 732: Loss 0.7164907072942128\n",
      "Epoch 733: Loss 0.7140332332161241\n",
      "Epoch 734: Loss 0.7133798823653075\n",
      "Epoch 735: Loss 0.7136309998361361\n",
      "Epoch 736: Loss 0.7158287528514279\n",
      "Epoch 737: Loss 0.7160132869826203\n",
      "Epoch 738: Loss 0.7157614504997579\n",
      "Epoch 739: Loss 0.7136683658664894\n",
      "Epoch 740: Loss 0.7150004152381876\n",
      "Epoch 741: Loss 0.7158839383407213\n",
      "Epoch 742: Loss 0.7169642448099212\n",
      "Epoch 743: Loss 0.7158104874373982\n",
      "Epoch 744: Loss 0.7156091920541726\n",
      "Epoch 745: Loss 0.7135891764806758\n",
      "Epoch 746: Loss 0.7129982485594115\n",
      "Epoch 747: Loss 0.7133812243060581\n",
      "Epoch 748: Loss 0.7136426623256347\n",
      "Epoch 749: Loss 0.7142294550427315\n",
      "Epoch 750: Loss 0.7154688014464432\n",
      "Epoch 751: Loss 0.712611523304117\n",
      "Epoch 752: Loss 0.7132028260825447\n",
      "Epoch 753: Loss 0.7139800581438123\n",
      "Epoch 754: Loss 0.7134067001689655\n",
      "Epoch 755: Loss 0.7146120832080279\n",
      "Epoch 756: Loss 0.7175190465822794\n",
      "Epoch 757: Loss 0.7128419976172748\n",
      "Epoch 758: Loss 0.7138261934370055\n",
      "Epoch 759: Loss 0.7134190347267674\n",
      "Epoch 760: Loss 0.7137147649756184\n",
      "Epoch 761: Loss 0.713290491552004\n",
      "Epoch 762: Loss 0.7157551055027844\n",
      "Epoch 763: Loss 0.7125792398150272\n",
      "Epoch 764: Loss 0.7140672310240374\n",
      "Epoch 765: Loss 0.7142752678287445\n",
      "Epoch 766: Loss 0.7173112508940614\n",
      "Epoch 767: Loss 0.7125400471102883\n",
      "Epoch 768: Loss 0.7158417126342944\n",
      "Epoch 769: Loss 0.7139311787985976\n",
      "Epoch 770: Loss 0.7124536435250519\n",
      "Epoch 771: Loss 0.7130847138832138\n",
      "Epoch 772: Loss 0.7134630892087896\n",
      "Epoch 773: Loss 0.7144508808181526\n",
      "Epoch 774: Loss 0.7151319907378761\n",
      "Epoch 775: Loss 0.7165129026098801\n",
      "Epoch 776: Loss 0.71326763100965\n",
      "Epoch 777: Loss 0.7153965954363902\n",
      "Epoch 778: Loss 0.7135430095082608\n",
      "Epoch 779: Loss 0.7128259878752212\n",
      "Epoch 780: Loss 0.7120549394229307\n",
      "Epoch 781: Loss 0.713014438623869\n",
      "Epoch 782: Loss 0.7119285421715152\n",
      "Epoch 783: Loss 0.7129501958189184\n",
      "Epoch 784: Loss 0.714293988878166\n",
      "Epoch 785: Loss 0.7118023481702065\n",
      "Epoch 786: Loss 0.7118815980659886\n",
      "Epoch 787: Loss 0.7120656717709795\n",
      "Epoch 788: Loss 0.7161446534389309\n",
      "Epoch 789: Loss 0.7117477001512192\n",
      "Epoch 790: Loss 0.7131362742288818\n",
      "Epoch 791: Loss 0.7113530065151233\n",
      "Epoch 792: Loss 0.7122071272311943\n",
      "Epoch 793: Loss 0.7129459493056918\n",
      "Epoch 794: Loss 0.7121418335732143\n",
      "Epoch 795: Loss 0.7119993645942413\n",
      "Epoch 796: Loss 0.7138050044889839\n",
      "Epoch 797: Loss 0.7134326634668512\n",
      "Epoch 798: Loss 0.7147284108972793\n",
      "Epoch 799: Loss 0.711894047036372\n",
      "Epoch 800: Loss 0.7109541567287512\n",
      "Epoch 801: Loss 0.7121202246087937\n",
      "Epoch 802: Loss 0.7135540300721943\n",
      "Epoch 803: Loss 0.7120167552368334\n",
      "Epoch 804: Loss 0.7132714220953309\n",
      "Epoch 805: Loss 0.712216648612232\n",
      "Epoch 806: Loss 0.7115997857438183\n",
      "Epoch 807: Loss 0.71258330892409\n",
      "Epoch 808: Loss 0.7116244848877866\n",
      "Epoch 809: Loss 0.7119400890459286\n",
      "Epoch 810: Loss 0.7157577183235033\n",
      "Epoch 811: Loss 0.7126698338140333\n",
      "Epoch 812: Loss 0.7126930837697983\n",
      "Epoch 813: Loss 0.7114422582914103\n",
      "Epoch 814: Loss 0.7103994196763133\n",
      "Epoch 815: Loss 0.7121553718125399\n",
      "Epoch 816: Loss 0.7120927904001882\n",
      "Epoch 817: Loss 0.711061282003756\n",
      "Epoch 818: Loss 0.7124907041437507\n",
      "Epoch 819: Loss 0.7124944159287513\n",
      "Epoch 820: Loss 0.7167002971003074\n",
      "Epoch 821: Loss 0.712976516406832\n",
      "Epoch 822: Loss 0.7100439796830426\n",
      "Epoch 823: Loss 0.7126963656719708\n",
      "Epoch 824: Loss 0.71188132715378\n",
      "Epoch 825: Loss 0.7103321223216813\n",
      "Epoch 826: Loss 0.7127106043157451\n",
      "Epoch 827: Loss 0.7107630140717213\n",
      "Epoch 828: Loss 0.71096863600352\n",
      "Epoch 829: Loss 0.7111443311895982\n",
      "Epoch 830: Loss 0.7107610624820676\n",
      "Epoch 831: Loss 0.709826667102397\n",
      "Epoch 832: Loss 0.7114050240145299\n",
      "Epoch 833: Loss 0.7101155774690031\n",
      "Epoch 834: Loss 0.7097294392683465\n",
      "Epoch 835: Loss 0.710789766877149\n",
      "Epoch 836: Loss 0.71245254624916\n",
      "Epoch 837: Loss 0.7100624384563085\n",
      "Epoch 838: Loss 0.7093165077214054\n",
      "Epoch 839: Loss 0.7110496591597796\n",
      "Epoch 840: Loss 0.7095814908263721\n",
      "Epoch 841: Loss 0.7097098485816199\n",
      "Epoch 842: Loss 0.7094532185820691\n",
      "Epoch 843: Loss 0.7095495576841263\n",
      "Epoch 844: Loss 0.7122858679186165\n",
      "Epoch 845: Loss 0.7116125439765401\n",
      "Epoch 846: Loss 0.7099313975666569\n",
      "Epoch 847: Loss 0.7091937999555465\n",
      "Epoch 848: Loss 0.711618240013668\n",
      "Epoch 849: Loss 0.7109106478744005\n",
      "Epoch 850: Loss 0.7123504082162017\n",
      "Epoch 851: Loss 0.7102078352463739\n",
      "Epoch 852: Loss 0.7093633574222497\n",
      "Epoch 853: Loss 0.7098186684950614\n",
      "Epoch 854: Loss 0.7088600020905312\n",
      "Epoch 855: Loss 0.7096557831604409\n",
      "Epoch 856: Loss 0.7122665293187118\n",
      "Epoch 857: Loss 0.7106121128172452\n",
      "Epoch 858: Loss 0.7094149343054863\n",
      "Epoch 859: Loss 0.7103458117244515\n",
      "Epoch 860: Loss 0.7087276605590284\n",
      "Epoch 861: Loss 0.7097881097496042\n",
      "Epoch 862: Loss 0.7088386300199668\n",
      "Epoch 863: Loss 0.7089318519140999\n",
      "Epoch 864: Loss 0.7092771625987864\n",
      "Epoch 865: Loss 0.7096017908672553\n",
      "Epoch 866: Loss 0.7111821591881078\n",
      "Epoch 867: Loss 0.7097098167342034\n",
      "Epoch 868: Loss 0.7104376568248986\n",
      "Epoch 869: Loss 0.7094723421453302\n",
      "Epoch 870: Loss 0.7098582675307654\n",
      "Epoch 871: Loss 0.7086196245283133\n",
      "Epoch 872: Loss 0.709392146138225\n",
      "Epoch 873: Loss 0.7112914691119452\n",
      "Epoch 874: Loss 0.7129326086498055\n",
      "Epoch 875: Loss 0.7140148693663491\n",
      "Epoch 876: Loss 0.7081059129871045\n",
      "Epoch 877: Loss 0.7093663229969688\n",
      "Epoch 878: Loss 0.7114469916893644\n",
      "Epoch 879: Loss 0.7084450005528822\n",
      "Epoch 880: Loss 0.7094906423972004\n",
      "Epoch 881: Loss 0.7079662295993372\n",
      "Epoch 882: Loss 0.707854608313115\n",
      "Epoch 883: Loss 0.7089243255573865\n",
      "Epoch 884: Loss 0.7104016555201257\n",
      "Epoch 885: Loss 0.7091743642254329\n",
      "Epoch 886: Loss 0.7108982358653664\n",
      "Epoch 887: Loss 0.7119544314502194\n",
      "Epoch 888: Loss 0.7099970274014027\n",
      "Epoch 889: Loss 0.7086478804508018\n",
      "Epoch 890: Loss 0.7096320188135046\n",
      "Epoch 891: Loss 0.7121246772717846\n",
      "Epoch 892: Loss 0.7084095875158709\n",
      "Epoch 893: Loss 0.7086753043940848\n",
      "Epoch 894: Loss 0.7078354386535233\n",
      "Epoch 895: Loss 0.7084514384746036\n",
      "Epoch 896: Loss 0.7094580452319051\n",
      "Epoch 897: Loss 0.7088193047217368\n",
      "Epoch 898: Loss 0.7105092402718572\n",
      "Epoch 899: Loss 0.7100083891323856\n",
      "Epoch 900: Loss 0.7090040169719507\n",
      "Epoch 901: Loss 0.708828145871691\n",
      "Epoch 902: Loss 0.7115346132202506\n",
      "Epoch 903: Loss 0.7087065820400185\n",
      "Epoch 904: Loss 0.7092367771345921\n",
      "Epoch 905: Loss 0.7083454840781069\n",
      "Epoch 906: Loss 0.7081666704510949\n",
      "Epoch 907: Loss 0.7079297962082867\n",
      "Epoch 908: Loss 0.7109520120076827\n",
      "Epoch 909: Loss 0.710849119144834\n",
      "Epoch 910: Loss 0.7083146738464979\n",
      "Epoch 911: Loss 0.708822481953938\n",
      "Epoch 912: Loss 0.7101254608322801\n",
      "Epoch 913: Loss 0.7079903879015176\n",
      "Epoch 914: Loss 0.7099574096400968\n",
      "Epoch 915: Loss 0.7072458799137498\n",
      "Epoch 916: Loss 0.7081007562597329\n",
      "Epoch 917: Loss 0.7113940711558228\n",
      "Epoch 918: Loss 0.709922582363192\n",
      "Epoch 919: Loss 0.7081968111401649\n",
      "Epoch 920: Loss 0.7081495260344837\n",
      "Epoch 921: Loss 0.7100783640009235\n",
      "Epoch 922: Loss 0.7086156208892902\n",
      "Epoch 923: Loss 0.7078518658599102\n",
      "Epoch 924: Loss 0.7087249751288764\n",
      "Epoch 925: Loss 0.7067287197590013\n",
      "Epoch 926: Loss 0.7069579148602217\n",
      "Epoch 927: Loss 0.7072218676836113\n",
      "Epoch 928: Loss 0.7098707100977693\n",
      "Epoch 929: Loss 0.709026492145959\n",
      "Epoch 930: Loss 0.709191785916579\n",
      "Epoch 931: Loss 0.7064398300368614\n",
      "Epoch 932: Loss 0.7066056977193108\n",
      "Epoch 933: Loss 0.7070888972758785\n",
      "Epoch 934: Loss 0.7072787442690704\n",
      "Epoch 935: Loss 0.7075670138360931\n",
      "Epoch 936: Loss 0.708864409423101\n",
      "Epoch 937: Loss 0.7081625227642109\n",
      "Epoch 938: Loss 0.7102189816379579\n",
      "Epoch 939: Loss 0.7070180765326841\n",
      "Epoch 940: Loss 0.7079473396789746\n",
      "Epoch 941: Loss 0.7082322948363737\n",
      "Epoch 942: Loss 0.7072081092126975\n",
      "Epoch 943: Loss 0.7069981883153313\n",
      "Epoch 944: Loss 0.7073591411008011\n",
      "Epoch 945: Loss 0.7080475011357398\n",
      "Epoch 946: Loss 0.7075055469415291\n",
      "Epoch 947: Loss 0.709975728556271\n",
      "Epoch 948: Loss 0.7082582555778492\n",
      "Epoch 949: Loss 0.7065550531127102\n",
      "Epoch 950: Loss 0.7075077254764724\n",
      "Epoch 951: Loss 0.7096708618032234\n",
      "Epoch 952: Loss 0.7072281892258652\n",
      "Epoch 953: Loss 0.7065064047175292\n",
      "Epoch 954: Loss 0.706490462700106\n",
      "Epoch 955: Loss 0.7064829295625438\n",
      "Epoch 956: Loss 0.7060112552371689\n",
      "Epoch 957: Loss 0.7077228401623676\n",
      "Epoch 958: Loss 0.7056117503311558\n",
      "Epoch 959: Loss 0.7071214737895591\n",
      "Epoch 960: Loss 0.7062061020766188\n",
      "Epoch 961: Loss 0.7078463320818118\n",
      "Epoch 962: Loss 0.7069576110198926\n",
      "Epoch 963: Loss 0.7067018984408273\n",
      "Epoch 964: Loss 0.7065196119157359\n",
      "Epoch 965: Loss 0.7072988557890247\n",
      "Epoch 966: Loss 0.7069480610068433\n",
      "Epoch 967: Loss 0.7068469545924613\n",
      "Epoch 968: Loss 0.7079720147053462\n",
      "Epoch 969: Loss 0.7059795713944893\n",
      "Epoch 970: Loss 0.7060643427126829\n",
      "Epoch 971: Loss 0.7091702793700527\n",
      "Epoch 972: Loss 0.7067045433093332\n",
      "Epoch 973: Loss 0.7075053415438688\n",
      "Epoch 974: Loss 0.7072580938573235\n",
      "Epoch 975: Loss 0.7061362687538738\n",
      "Epoch 976: Loss 0.7078590537286891\n",
      "Epoch 977: Loss 0.7056035771779622\n",
      "Epoch 978: Loss 0.706643601659138\n",
      "Epoch 979: Loss 0.7055579041747937\n",
      "Epoch 980: Loss 0.70648501236693\n",
      "Epoch 981: Loss 0.7088184563786454\n",
      "Epoch 982: Loss 0.7060970723497824\n",
      "Epoch 983: Loss 0.7052423878830948\n",
      "Epoch 984: Loss 0.7119546418871623\n",
      "Epoch 985: Loss 0.7058719381419267\n",
      "Epoch 986: Loss 0.7062691014596433\n",
      "Epoch 987: Loss 0.7063908788194597\n",
      "Epoch 988: Loss 0.7048601598283247\n",
      "Epoch 989: Loss 0.7067889196065805\n",
      "Epoch 990: Loss 0.7078179584934443\n",
      "Epoch 991: Loss 0.7060403085989129\n",
      "Epoch 992: Loss 0.7091398030554424\n",
      "Epoch 993: Loss 0.7069250220532183\n",
      "Epoch 994: Loss 0.7049756643447234\n",
      "Epoch 995: Loss 0.711122125756803\n",
      "Epoch 996: Loss 0.7077123001095318\n",
      "Epoch 997: Loss 0.7055116011079121\n",
      "Epoch 998: Loss 0.7049122083394365\n",
      "Epoch 999: Loss 0.7046843471411031\n",
      "0.6714669172779624 0.672650899767345 0.6715737020926806\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nn.train(X_train, y_train_onehot, 0.01, 100, 32)\n",
    "\n",
    "predictions = nn(X_test)\n",
    "\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "actual_classes = np.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "# Compute precision, recall, and F1 scores for each class\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(actual_classes, predicted_classes, average='macro')\n",
    "\n",
    "print(precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(hidden_layer_size, X_train, y_train_onehot, X_test, y_test_onehot):\n",
    "    nn = NN([\n",
    "        SigmoidLayer(1024, hidden_layer_size),\n",
    "        SoftMaxLayer(hidden_layer_size, 5)\n",
    "    ])\n",
    "    \n",
    "    nn.train(X_train, y_train_onehot, 0.01, 100, 32)\n",
    "\n",
    "    predictions = nn(X_test)\n",
    "\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    actual_classes = np.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "    # Compute precision, recall, and F1 scores for each class\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(actual_classes, predicted_classes, average='macro')\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [1, 5, 10, 50, 100]\n",
    "\n",
    "results = []\n",
    "\n",
    "with multiprocessing.Pool(processes=len(hidden_layer_sizes)) as pool:\n",
    "    results = pool.starmap(train_and_evaluate, [(size, X_train, y_train_onehot, X_test, y_test_onehot) for size in hidden_layer_sizes])\n",
    "\n",
    "precision, recall, f1 = zip(*results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(hidden_layer_sizes, precision, label='Precision')\n",
    "plt.plot(hidden_layer_sizes, recall, label='Recall')\n",
    "plt.plot(hidden_layer_sizes, f1, label='F1 Score')\n",
    "\n",
    "plt.xlabel('Hidden Layer Size')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.title('Precision, Recall, and F1 Score vs. Hidden Layer Size')\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "733",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
